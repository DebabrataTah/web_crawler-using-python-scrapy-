# ---some intro to web_crawler---
  What if we have an ecological study, but the data we need is not available to us? What if we want to validate our measure by comparing our estimates with external sources? what would we do?
Well one could go and got the data online. Web scraping/crawling(web harvesting or web data extraction) is a computer software technique that allows us to extract information data from various websites. when we want to extract data from a document, we would copy(ctrl+c) and paste(ctrl+v) the the document. For a website it is little tricky bcz of the way of information is formatted and stored, typically as HTML code. Thus scrapers/crawlers work by parsing the HTML source code of a website in order to extract and retrieve specific element within the page's code.
Search engines use a specific type of scraper, called web crawler or search bot, to crawl through the web pages and indentify which site they link to and wha terms they use. #Google and #Facebook really brought scraping to another level(' https://www.bbc.com/news/technology-23988890 '). Google scraped the web to catalogue all of the information on the internet and make it accessible. Recently, Facebook has been using scrappers to help people find connections and filled up their social networks.

#---diff between scrapper and crawler---
  1. Scraping is the process of processing web documents and extracting information out of it. 
  2. Crawling is the process of iteratively finding and fetching web links starting from a list of URL'S
